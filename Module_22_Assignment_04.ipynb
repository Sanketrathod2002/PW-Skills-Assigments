{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.**\n",
        "\n",
        "**Ans :**\n",
        "Min-Max scaling is a data preprocessing technique used to scale numeric features to a fixed range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the difference between the maximum and minimum values. Min-Max scaling is helpful when dealing with features that have different scales and ensures that all features contribute equally to the model."
      ],
      "metadata": {
        "id": "mhUCB2kyyu0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = [[1], [5], [10], [15], [20]]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Scaled data using Min-Max scaling:\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97R_dCC_y3v9",
        "outputId": "2b173377-036f-4121-e9a9-332198cf3ffc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled data using Min-Max scaling:\n",
            "[[0.        ]\n",
            " [0.21052632]\n",
            " [0.47368421]\n",
            " [0.73684211]\n",
            " [1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**\n",
        "\n",
        "**Ans :** he Unit Vector technique scales features to have a unit norm, meaning their magnitude becomes 1. It differs from Min-Max scaling in that it doesn't necessarily bound the features to a specific range. Unit Vector scaling is useful when the direction of the data matters more than its magnitude.\n"
      ],
      "metadata": {
        "id": "ae5nZ8ucywKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "scaler = Normalizer()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(\"Scaled data using Unit Vector scaling:\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WghBV_X7zCbV",
        "outputId": "eec98b23-17c3-4295-ada9-c8f31f473364"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled data using Unit Vector scaling:\n",
            "[[0.26726124 0.53452248 0.80178373]\n",
            " [0.45584231 0.56980288 0.68376346]\n",
            " [0.50257071 0.57436653 0.64616234]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**\n",
        "\n",
        "**Ans :**\n",
        "PCA is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving most of the information. It achieves this by identifying the directions (principal components) that capture the maximum variance in the data and projecting the data onto these components. PCA is useful for visualizing high-dimensional data and removing redundant features."
      ],
      "metadata": {
        "id": "haukR53czC_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "transformed_data = pca.fit_transform(data)\n",
        "\n",
        "print(\"Transformed data after PCA:\")\n",
        "print(transformed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92gAsCEBzHrU",
        "outputId": "00e76177-3a56-46af-a498-598a969e5f23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed data after PCA:\n",
            "[[-5.19615242e+00  2.56395025e-16]\n",
            " [ 0.00000000e+00  0.00000000e+00]\n",
            " [ 5.19615242e+00  2.56395025e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**\n",
        "\n",
        "**Ans :**\n",
        "PCA can be used for feature extraction by transforming the original features into a new set of orthogonal features (principal components) that capture the most significant variations in the data. These principal components can then be used as the new features for modeling. Feature extraction using PCA can help reduce the dimensionality of the dataset while retaining most of the information."
      ],
      "metadata": {
        "id": "xlXayMXez97T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "transformed_data = pca.fit_transform(data)\n",
        "\n",
        "print(\"Transformed data after PCA (used for feature extraction):\")\n",
        "print(transformed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EYOWYqC0HvG",
        "outputId": "868ef299-81c3-4f68-9119-94c296c95abb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed data after PCA (used for feature extraction):\n",
            "[[-5.19615242e+00  2.56395025e-16]\n",
            " [ 0.00000000e+00  0.00000000e+00]\n",
            " [ 5.19615242e+00  2.56395025e-16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**\n",
        "\n",
        "**Ans :**\n",
        "To use Min-Max scaling for preprocessing:\n",
        "- Calculate the minimum and maximum values for each feature (e.g., price, rating, delivery time).\n",
        "- Subtract the minimum value from each feature and divide by the difference between the maximum and minimum values.\n",
        "- This transforms the values of each feature to a range between 0 and 1, making them comparable and suitable for modeling."
      ],
      "metadata": {
        "id": "UiMkfyiL0IWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "price = [10, 20, 30, 40]\n",
        "rating = [3.5, 4.2, 4.8, 3.9]\n",
        "delivery_time = [25, 30, 20, 35]\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "price_scaled = scaler.fit_transform([[p] for p in price])\n",
        "rating_scaled = scaler.fit_transform([[r] for r in rating])\n",
        "delivery_time_scaled = scaler.fit_transform([[dt] for dt in delivery_time])\n",
        "\n",
        "print(\"Scaled price:\", price_scaled)\n",
        "print(\"Scaled rating:\", rating_scaled)\n",
        "print(\"Scaled delivery time:\", delivery_time_scaled)"
      ],
      "metadata": {
        "id": "ro_946-d0RwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**\n",
        "\n",
        "**Ans :**\n",
        "To use PCA for dimensionality reduction:\n",
        "- Standardize the features to have zero mean and unit variance.\n",
        "- Apply PCA to the standardized dataset to find the principal components that capture the most significant variations in the data.\n",
        "- Select a reduced number of principal components that explain a high percentage of the variance in the data, effectively reducing the dimensionality while retaining most of the information."
      ],
      "metadata": {
        "id": "A46PFPHV0UYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "price = [10, 20, 30, 40]\n",
        "rating = [3.5, 4.2, 4.8, 3.9]\n",
        "delivery_time = [25, 30, 20, 35]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "price_scaled = scaler.fit_transform([[p] for p in price])\n",
        "rating_scaled = scaler.fit_transform([[r] for r in rating])\n",
        "delivery_time_scaled = scaler.fit_transform([[dt] for dt in delivery_time])\n",
        "\n",
        "print(\"Scaled price:\", price_scaled)\n",
        "print(\"Scaled rating:\", rating_scaled)\n",
        "print(\"Scaled delivery time:\", delivery_time_scaled)"
      ],
      "metadata": {
        "id": "VDUxXYTL0ZOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**\n",
        "\n",
        "**Ans :**\n",
        "Min-Max scaling is a method used to scale the values of a dataset to a specified range. To perform Min-Max scaling, we subtract the minimum value of the dataset and then divide by the difference between the maximum and minimum values. Finally, we multiply by the desired range and add the minimum of the range."
      ],
      "metadata": {
        "id": "z0LVERdu0zj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 5, 10, 15, 20])\n",
        "\n",
        "min_val = np.min(data)\n",
        "max_val = np.max(data)\n",
        "scaled_data = -1 + (data - min_val) * (2 / (max_val - min_val))\n",
        "\n",
        "print(\"Scaled data using Min-Max scaling to range of -1 to 1:\")\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAxmxkTt03Ud",
        "outputId": "76507e29-e53c-4994-d206-e4074df9b5c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled data using Min-Max scaling to range of -1 to 1:\n",
            "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**\n",
        "\n",
        "**Ans :**\n",
        "PCA can be used for feature extraction by finding the principal components that capture the most significant variations in the data. To determine the number of principal components to retain, we typically look at the cumulative explained variance ratio. Retaining enough principal components to explain a high percentage (e.g., 95%) of the total variance ensures that most of the information in the data is preserved while reducing dimensionality."
      ],
      "metadata": {
        "id": "Of0NM0LR05ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = np.random.rand(100, 5)  # Assuming 100 samples and 5 features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "standardized_features = scaler.fit_transform(features)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(standardized_features)\n",
        "\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
        "\n",
        "print(\"Number of principal components to retain:\", num_components)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVHY60s008sw",
        "outputId": "22f23d29-45c3-421e-cc42-9d3942bc38fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of principal components to retain: 5\n"
          ]
        }
      ]
    }
  ]
}