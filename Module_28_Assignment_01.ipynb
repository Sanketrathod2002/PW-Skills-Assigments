{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is the KNN algorithm?**\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm is a non-parametric and lazy learning algorithm used for both classification and regression tasks in machine learning. It makes predictions based on the similarity of data points. Given a new data point, KNN identifies the K nearest data points (neighbors) in the training set and assigns the most common class label (for classification) or the average of the target values (for regression) among those neighbors to the new data point.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. How do you choose the value of K in KNN?**\n",
        "\n",
        "The choice of the value of K in KNN significantly affects the model's performance. Typically, you choose K through hyperparameter tuning, where you evaluate the model's performance using different values of K and select the one that provides the best results on a validation set or through cross-validation. A smaller value of K (e.g., K=1) tends to have higher variance and can lead to overfitting, while a larger value of K can introduce more bias.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. What is the difference between KNN classifier and KNN regressor?**\n",
        "\n",
        "- **KNN Classifier**: In KNN classification, the algorithm predicts the class label of a new data point based on the majority class among its K nearest neighbors.\n",
        "- **KNN Regressor**: In KNN regression, the algorithm predicts the target value of a new data point by averaging the target values of its K nearest neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. How do you measure the performance of KNN?**\n",
        "\n",
        "The performance of KNN can be measured using various evaluation metrics, depending on whether it's a classification or regression task. For classification, common metrics include accuracy, precision, recall, F1-score, and ROC-AUC. For regression, common metrics include mean squared error (MSE), mean absolute error (MAE), R-squared, and root mean squared error (RMSE).\n",
        "\n",
        "---\n",
        "\n",
        "**Q5. What is the curse of dimensionality in KNN?**\n",
        "\n",
        "The curse of dimensionality refers to the phenomena where the performance of KNN deteriorates as the number of features (dimensions) increases. In high-dimensional spaces, the distance between data points becomes less meaningful, leading to all data points being approximately equidistant from each other. This makes it difficult for KNN to find meaningful neighbors, affecting its performance adversely.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6. How do you handle missing values in KNN?**\n",
        "\n",
        "One approach to handling missing values in KNN is to impute them before applying the algorithm. Common imputation methods include replacing missing values with the mean, median, or mode of the feature, or using more advanced techniques such as k-nearest neighbors imputation or matrix completion techniques.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
        "\n",
        "- **KNN Classifier**: KNN classifier is suitable for classification problems where the decision boundary is non-linear and the classes are well-separated. It tends to perform well when the dataset is small to medium-sized and does not have a large number of features.\n",
        "- **KNN Regressor**: KNN regressor is suitable for regression problems where the relationship between the features and the target variable is non-linear and locally smooth. It can be effective when the dataset has a moderate number of features and instances.\n",
        "\n",
        "---\n",
        "\n",
        "**Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
        "\n",
        "- **Strengths**:\n",
        "  - Simple to understand and implement.\n",
        "  - No assumptions about the underlying data distribution.\n",
        "  - Effective in capturing local patterns in the data.\n",
        "\n",
        "- **Weaknesses**:\n",
        "  - Computationally expensive, especially with large datasets.\n",
        "  - Sensitive to the choice of K and the distance metric.\n",
        "  - Requires sufficient and relevant training data to make accurate predictions.\n",
        "\n",
        "To address these weaknesses, one can use techniques such as dimensionality reduction, feature scaling, and optimizing hyperparameters through cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "**Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
        "\n",
        "- **Euclidean Distance**: Euclidean distance is the straight-line distance between two points in Euclidean space. It is computed as the square root of the sum of the squared differences between corresponding coordinates of two points.\n",
        "- **Manhattan Distance**: Manhattan distance, also known as city block distance or L1 distance, is the sum of the absolute differences between corresponding coordinates of two points. It measures the distance traveled along axis-aligned paths.\n",
        "\n",
        "---\n",
        "\n",
        "**Q10. What is the role of feature scaling in KNN?**\n",
        "\n",
        "Feature scaling is essential in KNN because the algorithm computes distances between data points to determine their similarity. If the features are on different scales, features with larger magnitudes may dominate the distance calculation. Feature scaling ensures that all features contribute equally to the distance calculation, preventing bias towards features with larger scales and improving the performance of the algorithm. Common scaling techniques include min-max scaling, z-score normalization, and robust scaling.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LIwDgwDHUE9r"
      }
    }
  ]
}