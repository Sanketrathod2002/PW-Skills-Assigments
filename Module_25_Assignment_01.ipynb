{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. **Difference between Simple Linear Regression and Multiple Linear Regression:**\n",
        "\n",
        "Simple Linear Regression involves predicting a dependent variable based on one independent variable, while Multiple Linear Regression involves predicting a dependent variable based on two or more independent variables.\n",
        "\n",
        "Example of Simple Linear Regression:\n",
        "Suppose we want to predict the sales of a product based on advertising expenditure on TV. Here, we have one independent variable (TV advertising expenditure) and one dependent variable (sales).\n",
        "\n",
        "Example of Multiple Linear Regression:\n",
        "Suppose we want to predict the sales of a product based on advertising expenditure on TV, radio, and newspaper. Here, we have three independent variables (TV advertising expenditure, radio advertising expenditure, newspaper advertising expenditure) and one dependent variable (sales).\n",
        "\n",
        "---\n",
        "\n",
        "Q2. **Assumptions of Linear Regression and Checking for Them:**\n",
        "\n",
        "Assumptions include linearity, independence, homoscedasticity (constant variance of errors), and normality of errors. You can check these assumptions through diagnostic plots (such as residuals vs. fitted values plot) and statistical tests (like Shapiro-Wilk test for normality).\n",
        "\n",
        "---\n",
        "\n",
        "Q3. **Interpretation of Slope and Intercept in Linear Regression:**\n",
        "\n",
        "The slope represents the change in the dependent variable for a one-unit change in the independent variable, while the intercept represents the value of the dependent variable when all independent variables are zero.\n",
        "\n",
        "Example: In a linear regression model predicting exam scores based on study hours, the slope indicates how much the exam score is expected to increase (or decrease) for each additional hour of study. The intercept represents the expected exam score when the study hours are zero.\n",
        "\n",
        "---\n",
        "\n",
        "Q4. **Concept of Gradient Descent and its Use in Machine Learning:**\n",
        "\n",
        "Gradient Descent is an optimization algorithm used to minimize the loss function in machine learning models. It works by iteratively adjusting the parameters of the model in the direction of the steepest descent of the loss function gradient. This helps in finding the optimal parameters that minimize the prediction error.\n",
        "\n",
        "---\n",
        "\n",
        "Q5. **Description of Multiple Linear Regression Model:**\n",
        "\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression where multiple independent variables are used to predict a dependent variable. It models the relationship between the dependent variable and multiple independent variables by fitting a linear equation to observed data.\n",
        "\n",
        "Difference from Simple Linear Regression:\n",
        "Simple Linear Regression involves only one independent variable, whereas Multiple Linear Regression involves two or more independent variables.\n",
        "\n",
        "---\n",
        "\n",
        "Q6. **Concept of Multicollinearity in Multiple Linear Regression and its Detection/Addressing:**\n",
        "\n",
        "Multicollinearity occurs when independent variables in a regression model are highly correlated. It can lead to unstable coefficient estimates and inaccurate hypothesis testing results. Detecting multicollinearity can be done through correlation matrices or variance inflation factor (VIF). Addressing multicollinearity can involve removing one of the correlated variables or using dimensionality reduction techniques.\n",
        "\n",
        "---\n",
        "\n",
        "Q7. **Description of Polynomial Regression Model and its Difference from Linear Regression:**\n",
        "\n",
        "Polynomial Regression is a form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. Unlike Linear Regression, which fits a straight line, Polynomial Regression can fit curves to the data.\n",
        "\n",
        "---\n",
        "\n",
        "Q8. **Advantages and Disadvantages of Polynomial Regression Compared to Linear Regression and Preferred Situations for Use:**\n",
        "\n",
        "Advantages of Polynomial Regression include its ability to capture non-linear relationships between variables. However, it can suffer from overfitting, especially with higher degree polynomials, and may be computationally expensive. Polynomial Regression is preferred when the relationship between variables is non-linear and cannot be adequately captured by Linear Regression.\n",
        "---"
      ],
      "metadata": {
        "id": "TQKmOUdwKv_k"
      }
    }
  ]
}