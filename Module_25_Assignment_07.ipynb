{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. **Difference between Linear Regression and Logistic Regression:**\n",
        "   - Linear regression is used for predicting continuous numerical outcomes, while logistic regression is used for predicting categorical outcomes, specifically binary outcomes (0 or 1).\n",
        "   - Example: Linear regression could be used to predict house prices based on features like square footage and number of bedrooms, while logistic regression could be used to predict whether a customer will churn (leave) a subscription service based on their demographic and usage data.\n",
        "\n",
        "---\n",
        "\n",
        "Q2. **Cost Function and Optimization in Logistic Regression:**\n",
        "   - The cost function used in logistic regression is the logistic loss (or cross-entropy loss), which measures the difference between the predicted probabilities and the actual binary outcomes.\n",
        "   - Optimization techniques such as gradient descent or more advanced algorithms like L-BFGS are commonly used to minimize the cost function and find the optimal parameters (coefficients) of the logistic regression model.\n",
        "\n",
        "---\n",
        "\n",
        "Q3. **Regularization in Logistic Regression and its Role in Preventing Overfitting:**\n",
        "   - Regularization in logistic regression involves adding a penalty term to the cost function to discourage overly complex models with large coefficient values.\n",
        "   - Techniques like L1 (Lasso) and L2 (Ridge) regularization are commonly used to prevent overfitting by penalizing large coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "Q4. **ROC Curve and its Use in Evaluating Logistic Regression Model Performance:**\n",
        "   - The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across various threshold settings.\n",
        "   - It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) for different threshold values.\n",
        "   - The area under the ROC curve (AUC-ROC) is a common metric used to evaluate the overall performance of the logistic regression model, with higher AUC indicating better discrimination between the positive and negative classes.\n",
        "\n",
        "---\n",
        "\n",
        "Q5. **Common Techniques for Feature Selection in Logistic Regression:**\n",
        "   - Some common techniques for feature selection in logistic regression include:\n",
        "     - Forward selection: Starting with an empty set of features and adding the most predictive feature at each step.\n",
        "     - Backward elimination: Starting with all features and removing the least predictive feature at each step.\n",
        "     - L1 regularization (Lasso): Encouraging sparsity in the coefficients, effectively performing feature selection by setting some coefficients to zero.\n",
        "   - These techniques help improve model performance by reducing overfitting and increasing model interpretability by focusing on the most informative features.\n",
        "\n",
        "---\n",
        "\n",
        "Q6. **Handling Imbalanced Datasets in Logistic Regression:**\n",
        "   - Techniques for handling imbalanced datasets in logistic regression include:\n",
        "     - Resampling techniques such as oversampling the minority class or undersampling the majority class.\n",
        "     - Using different performance metrics like precision, recall, F1-score, or using class weights to account for the class imbalance.\n",
        "     - Synthetic minority oversampling technique (SMOTE) to generate synthetic samples for the minority class.\n",
        "\n",
        "---\n",
        "\n",
        "Q7. **Common Issues and Challenges in Logistic Regression Implementation:**\n",
        "   - Multicollinearity among independent variables can lead to unstable coefficient estimates. Techniques to address multicollinearity include removing highly correlated variables, using regularization, or performing dimensionality reduction.\n",
        "   - Other issues include outliers, missing data, and non-linear relationships between features and the log-odds of the outcome, which may require data preprocessing or more advanced modeling techniques.\n",
        "\n",
        "   ---"
      ],
      "metadata": {
        "id": "OQblkEZF5zIA"
      }
    }
  ]
}