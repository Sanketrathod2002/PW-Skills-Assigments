{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. **Concept of R-squared in Linear Regression:**\n",
        "\n",
        "R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It is calculated as the ratio of the explained variance to the total variance.\n",
        "\n",
        "---\n",
        "\n",
        "Q2. **Definition of Adjusted R-squared and its Difference from R-squared:**\n",
        "\n",
        "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It penalizes the addition of unnecessary predictors to the model. Unlike R-squared, adjusted R-squared increases only if the new term improves the model more than would be expected by chance.\n",
        "\n",
        "---\n",
        "\n",
        "Q3. **Appropriateness of Adjusted R-squared:**\n",
        "\n",
        "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors, as it penalizes the inclusion of irrelevant predictors and provides a more accurate measure of model fit.\n",
        "\n",
        "---\n",
        "\n",
        "Q4. **Explanation of RMSE, MSE, and MAE in Regression Analysis:**\n",
        "\n",
        "- RMSE (Root Mean Squared Error) represents the square root of the average of squared differences between predicted and actual values.\n",
        "- MSE (Mean Squared Error) represents the average of squared differences between predicted and actual values.\n",
        "- MAE (Mean Absolute Error) represents the average of absolute differences between predicted and actual values.\n",
        "\n",
        "---\n",
        "\n",
        "Q5. **Advantages and Disadvantages of RMSE, MSE, and MAE:**\n",
        "\n",
        "Advantages include their simplicity and intuitive interpretation. However, they may not be robust to outliers, and RMSE and MSE give more weight to large errors.\n",
        "\n",
        "---\n",
        "\n",
        "Q6. **Explanation of Lasso Regularization and its Difference from Ridge Regularization:**\n",
        "\n",
        "Lasso regularization adds a penalty term to the loss function equal to the absolute value of the coefficients, while Ridge regularization adds a penalty term equal to the squared value of the coefficients. Lasso tends to shrink coefficients to zero, thus performing variable selection, whereas Ridge tends to shrink coefficients towards zero but rarely sets them exactly to zero.\n",
        "\n",
        "---\n",
        "\n",
        "Q7. **Role of Regularized Linear Models in Preventing Overfitting:**\n",
        "\n",
        "Regularized linear models penalize large coefficients, which helps to prevent overfitting by reducing the complexity of the model. For example, in Lasso regularization, some coefficients may be set to zero, effectively removing those features from the model and reducing overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "Q8. **Limitations of Regularized Linear Models:**\n",
        "\n",
        "Regularized linear models may not perform well if there are many irrelevant features or if the relationship between features and target variable is highly non-linear. Additionally, choosing the appropriate regularization parameter can be challenging.\n",
        "\n",
        "---\n",
        "\n",
        "Q9. **Comparison of Regression Models Based on Different Evaluation Metrics:**\n",
        "\n",
        "Model B with an MAE of 8 would be chosen as the better performer, as MAE directly measures the average magnitude of errors and is less sensitive to outliers compared to RMSE. However, limitations include MAE not considering the relative importance of errors.\n",
        "\n",
        "---\n",
        "\n",
        "Q10. **Comparison of Regularized Linear Models with Different Regularization Types:**\n",
        "\n",
        "The choice between Ridge and Lasso regularization depends on the specific problem and the desired outcome. In this case, the better performer would depend on the specific data and problem at hand. Ridge regularization may be preferred when all features are potentially relevant, while Lasso may be preferred for feature selection due to its ability to shrink coefficients to zero. However, Lasso may perform poorly if there are highly correlated features.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XgUg1neg23Tz"
      }
    }
  ]
}