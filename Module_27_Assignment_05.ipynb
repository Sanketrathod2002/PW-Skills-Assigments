{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Random Forest Regressor?**\n",
        "\n",
        "Random Forest Regressor is a machine learning algorithm that belongs to the ensemble learning category, specifically under the bagging method. It is an extension of the decision tree algorithm. Instead of building a single decision tree, the Random Forest Regressor builds a collection of decision trees (a forest), where each tree is trained on a random subset of the training data and features. The final prediction is made by aggregating the predictions of all trees in the forest.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. How does Random Forest Regressor reduce the risk of overfitting?**\n",
        "\n",
        "Random Forest Regressor reduces the risk of overfitting through the following mechanisms:\n",
        "1. **Bootstrap Sampling**: Each tree in the random forest is trained on a random subset of the training data, sampled with replacement. This randomness helps in reducing overfitting by introducing diversity among the trees.\n",
        "2. **Feature Randomness**: At each split of a decision tree, a random subset of features is considered for splitting. This ensures that no single feature dominates the decision-making process across all trees, reducing the chance of overfitting.\n",
        "3. **Ensemble Averaging**: By aggregating predictions from multiple trees, the random forest tends to generalize well on unseen data, as it combines the strengths of individual trees while mitigating their weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?**\n",
        "\n",
        "Random Forest Regressor aggregates the predictions of multiple decision trees by averaging the outputs of individual trees. In the case of regression tasks, the final prediction is typically the mean (average) of the predictions of all trees in the forest.\n",
        "\n",
        "**Q4. What are the hyperparameters of Random Forest Regressor?**\n",
        "\n",
        "Some common hyperparameters of the Random Forest Regressor include:\n",
        "- **n_estimators**: Number of trees in the forest.\n",
        "- **max_depth**: Maximum depth of each decision tree.\n",
        "- **min_samples_split**: Minimum number of samples required to split an internal node.\n",
        "- **min_samples_leaf**: Minimum number of samples required to be at a leaf node.\n",
        "- **max_features**: Number of features to consider when looking for the best split.\n",
        "- **bootstrap**: Whether bootstrap samples are used when building trees.\n",
        "\n",
        "---\n",
        "\n",
        "**Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?**\n",
        "\n",
        "The main differences between Random Forest Regressor and Decision Tree Regressor are:\n",
        "- Random Forest Regressor builds multiple decision trees and aggregates their predictions, whereas Decision Tree Regressor builds a single decision tree.\n",
        "- Random Forest Regressor introduces randomness in both data sampling and feature selection, reducing overfitting, while Decision Tree Regressor tends to overfit the training data.\n",
        "- Random Forest Regressor typically has better generalization performance than Decision Tree Regressor, especially on noisy datasets or datasets with high dimensionality.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6. What are the advantages and disadvantages of Random Forest Regressor?**\n",
        "\n",
        "**Advantages:**\n",
        "- Reduced risk of overfitting compared to individual decision trees.\n",
        "- Robust performance on various types of datasets, including those with missing values or outliers.\n",
        "- Can handle high-dimensional data well.\n",
        "- Provides feature importance scores.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Less interpretable compared to individual decision trees.\n",
        "- Can be computationally expensive, especially with large datasets and a large number of trees.\n",
        "- May not perform as well as other ensemble methods (e.g., gradient boosting) on certain types of datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7. What is the output of Random Forest Regressor?**\n",
        "\n",
        "The output of a Random Forest Regressor is a continuous numerical value, representing the predicted target variable for a given input instance. In regression tasks, the output is typically the mean (average) of the predictions of all trees in the forest.\n",
        "\n",
        "---\n",
        "\n",
        "**Q8. Can Random Forest Regressor be used for classification tasks?**\n",
        "\n",
        "Yes, Random Forest Regressor can also be used for classification tasks. In classification tasks, the Random Forest algorithm is known as Random Forest Classifier. It works similarly to the regressor but is adapted to predict class labels instead of continuous numerical values.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5_rwPcmDQm_j"
      }
    }
  ]
}