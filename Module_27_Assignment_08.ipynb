{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Gradient Boosting Regression?**\n",
        "\n",
        "Gradient Boosting Regression is a machine learning technique that builds an ensemble of weak regression models, typically decision trees, in a sequential manner. Unlike traditional boosting methods that focus on correcting errors by adjusting weights, gradient boosting fits each new model to the residual errors of the ensemble's predictions from the previous models. This process continues iteratively, with each new model aiming to reduce the errors made by the ensemble of previous models.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.**\n"
      ],
      "metadata": {
        "id": "K5yCIsHSR67G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 3, 4, 5, 6])\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        predictions = np.full_like(y, np.mean(y), dtype=np.float64)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Calculate residuals\n",
        "            residuals = y - predictions\n",
        "\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            self.estimators.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        predictions = np.zeros(len(X), dtype=np.float64)\n",
        "\n",
        "\n",
        "        for tree in self.estimators:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gb_regressor.fit(X, y)\n",
        "\n",
        "y_pred = gb_regressor.predict(X)\n",
        "\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "r_squared = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3Ew2qFHSFdo",
        "outputId": "e7d2b7d2-9d55-4a1f-cc83-17f31e072e3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 16.000000001411017\n",
            "R-squared: -7.0000000007055085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters.**"
      ],
      "metadata": {
        "id": "d5TITc1nSIcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([2, 3, 4, 5, 6])\n",
        "\n",
        "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        predictions = np.full_like(y, np.mean(y), dtype=np.float64)\n",
        "\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - predictions\n",
        "\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            self.estimators.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.zeros(len(X), dtype=np.float64)\n",
        "\n",
        "        for tree in self.estimators:\n",
        "            predictions += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\"n_estimators\": self.n_estimators,\n",
        "                \"learning_rate\": self.learning_rate,\n",
        "                \"max_depth\": self.max_depth}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [1, 2, 3]\n",
        "}\n",
        "\n",
        "gb_regressor = GradientBoostingRegressor()\n",
        "\n",
        "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X)\n",
        "mse = np.mean((y - y_pred) ** 2)\n",
        "r_squared = 1 - (np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
        "\n",
        "print(\"Best Model Mean Squared Error:\", mse)\n",
        "print(\"Best Model R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbr6P3tdSHFh",
        "outputId": "22259363-3a13-4510-e78c-dbe9a4cfbddd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 150}\n",
            "Best Model Mean Squared Error: 15.999999999999991\n",
            "Best Model R-squared: -6.999999999999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Q4. What is a weak learner in Gradient Boosting?**\n",
        "\n",
        "A weak learner in Gradient Boosting is a simple predictive model that performs slightly better than random guessing on a given task. In the context of regression, weak learners are typically shallow decision trees with limited depth. These weak learners are iteratively combined to form a strong predictive model through boosting, where each subsequent weak learner is trained to correct the errors made by the ensemble of previous weak learners.\n",
        "\n",
        "---\n",
        "**Q5. What is the intuition behind the Gradient Boosting algorithm?**\n",
        "\n",
        "The intuition behind the Gradient Boosting algorithm is to iteratively improve the model's predictive performance by focusing on the mistakes made by the ensemble of weak learners. It achieves this by fitting each new weak learner to the residuals (errors) of the previous model's predictions. By sequentially reducing the residuals, Gradient Boosting constructs an ensemble of weak learners that collectively form a strong predictive model.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?**\n",
        "\n",
        "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. It starts by fitting an initial weak learner to the data and then fits subsequent weak learners to the residuals (errors) of the ensemble's predictions from the previous models. Each weak learner is trained to minimize the loss function with respect to the residuals, effectively correcting the errors made by the ensemble of previous weak learners.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**\n",
        "\n",
        "The mathematical intuition of the Gradient Boosting algorithm involves the following steps:\n",
        "\n",
        "Initialize the ensemble's predictions with a constant value (e.g., mean of target variable).\n",
        "Calculate the residuals (errors) between the actual target values and the ensemble's predictions.\n",
        "Fit a weak learner (e.g., decision tree) to the residuals, aiming to minimize the loss function.\n",
        "Update the ensemble's predictions by adding a scaled version of the weak learner's predictions.\n",
        "Repeat steps 2-4 until a stopping criterion is met (e.g., maximum number of iterations reached or no significant improvement in performance).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5cBA6O2_TIUx"
      }
    }
  ]
}