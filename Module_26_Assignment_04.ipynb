{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
        "algorithms?**\n",
        "\n",
        "Polynomial functions and kernel functions are closely related in machine learning algorithms, particularly in Support Vector Machines (SVMs). Polynomial kernel functions are used to implicitly map input data into higher-dimensional feature spaces, where linear separation of classes might be possible. This mapping is achieved by computing the dot product between transformed feature vectors, effectively capturing non-linear relationships between the original features. Polynomial functions are often used as basis functions in kernel methods, allowing SVMs to learn complex decision boundaries by mapping data into higher-dimensional spaces.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FAFCG7nrFnbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?**"
      ],
      "metadata": {
        "id": "eW0kSSdrF5ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC(kernel='poly')\n",
        "\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nElTLvXHFz8b",
        "outputId": "80c9425f-5652-4dec-e3c7-1a43ff4d5621"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?**\n",
        "\n",
        "Increasing the value of epsilon in Support Vector Regression (SVR) typically leads to a larger margin of tolerance for errors. This means that data points within a distance of epsilon from the predicted value are not considered errors and do not contribute to the loss function during training. Consequently, as epsilon increases, more data points are treated as non-support vectors, resulting in fewer support vectors.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter worksand provide examples of when you might want to increase or decrease its value?**\n",
        "\n",
        "In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly affect the performance of the model:\n",
        "   - Kernel function: Different kernel functions (e.g., linear, polynomial, RBF) can capture different types of relationships in the data. The choice of kernel depends on the nature of the data and the problem at hand.\n",
        "   - C parameter: The C parameter controls the trade-off between maximizing the margin and minimizing the training error. Larger values of C allow for more flexibility in fitting the training data but may lead to overfitting.\n",
        "   - Epsilon parameter: The epsilon parameter defines the margin of tolerance for errors in SVR. Increasing epsilon allows for a wider margin of tolerance, which can lead to a smoother decision boundary and fewer support vectors.\n",
        "   - Gamma parameter: The gamma parameter determines the influence of a single training example. Higher values of gamma lead to a more complex decision boundary, potentially resulting in overfitting.\n",
        "\n",
        "   Adjusting these parameters requires careful tuning to achieve optimal performance. For example, increasing the C parameter can help improve the model's accuracy on the training data but may lead to overfitting if not regularized properly. Similarly, adjusting the epsilon parameter can control the smoothness of the regression function, affecting the trade-off between bias and variance.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qul_6Q9BGIw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Assignment:**\n",
        "L Import the necessary libraries and load the dataseg\n",
        "L Split the dataset into training and testing setZ\n",
        "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
        "L Create an instance of the SVC classifier and train it on the training datW\n",
        "L hse the trained classifier to predict the labels of the testing datW\n",
        "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
        "precision, recall, F1-scoreK\n",
        "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
        "improve its performanc_\n",
        "L Train the tuned classifier on the entire dataseg\n",
        "L Save the trained classifier to a file for future use."
      ],
      "metadata": {
        "id": "cs8qCH0hGhMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC()\n",
        "\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "tuned_classifier = grid_search.best_estimator_\n",
        "tuned_classifier.fit(X, y)\n",
        "\n",
        "joblib.dump(tuned_classifier, 'svm_classifier.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAoCYq2QGkWD",
        "outputId": "778e2cf2-237d-4009-8c14-60acca7a640a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_classifier.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}